{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meteostat import Point, Daily, Hourly\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import holidays\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Date et heure de comptage</th>\n",
       "      <th>Débit horaire</th>\n",
       "      <th>Taux d'occupation</th>\n",
       "      <th>Etat trafic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sts_Peres</td>\n",
       "      <td>2022-01-02 21:00:00</td>\n",
       "      <td>345.0</td>\n",
       "      <td>3.85500</td>\n",
       "      <td>Fluide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sts_Peres</td>\n",
       "      <td>2022-01-02 19:00:00</td>\n",
       "      <td>482.0</td>\n",
       "      <td>5.92944</td>\n",
       "      <td>Fluide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sts_Peres</td>\n",
       "      <td>2022-01-02 22:00:00</td>\n",
       "      <td>260.0</td>\n",
       "      <td>3.10555</td>\n",
       "      <td>Fluide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sts_Peres</td>\n",
       "      <td>2022-01-02 16:00:00</td>\n",
       "      <td>523.0</td>\n",
       "      <td>10.19222</td>\n",
       "      <td>Fluide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sts_Peres</td>\n",
       "      <td>2022-01-02 12:00:00</td>\n",
       "      <td>422.0</td>\n",
       "      <td>5.34889</td>\n",
       "      <td>Fluide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405</th>\n",
       "      <td>Convention</td>\n",
       "      <td>2022-10-31 13:00:00</td>\n",
       "      <td>685.0</td>\n",
       "      <td>3.70889</td>\n",
       "      <td>Fluide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8406</th>\n",
       "      <td>Convention</td>\n",
       "      <td>2022-08-01 01:00:00</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.68445</td>\n",
       "      <td>Fluide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8407</th>\n",
       "      <td>Convention</td>\n",
       "      <td>2022-08-01 00:00:00</td>\n",
       "      <td>226.0</td>\n",
       "      <td>1.09111</td>\n",
       "      <td>Fluide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8408</th>\n",
       "      <td>Convention</td>\n",
       "      <td>2022-07-31 23:00:00</td>\n",
       "      <td>267.0</td>\n",
       "      <td>1.47611</td>\n",
       "      <td>Fluide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8409</th>\n",
       "      <td>Convention</td>\n",
       "      <td>2022-08-01 02:00:00</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.52000</td>\n",
       "      <td>Fluide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25184 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Libelle Date et heure de comptage  Débit horaire  Taux d'occupation  \\\n",
       "0      Sts_Peres       2022-01-02 21:00:00          345.0            3.85500   \n",
       "1      Sts_Peres       2022-01-02 19:00:00          482.0            5.92944   \n",
       "2      Sts_Peres       2022-01-02 22:00:00          260.0            3.10555   \n",
       "3      Sts_Peres       2022-01-02 16:00:00          523.0           10.19222   \n",
       "4      Sts_Peres       2022-01-02 12:00:00          422.0            5.34889   \n",
       "...          ...                       ...            ...                ...   \n",
       "8405  Convention       2022-10-31 13:00:00          685.0            3.70889   \n",
       "8406  Convention       2022-08-01 01:00:00          140.0            0.68445   \n",
       "8407  Convention       2022-08-01 00:00:00          226.0            1.09111   \n",
       "8408  Convention       2022-07-31 23:00:00          267.0            1.47611   \n",
       "8409  Convention       2022-08-01 02:00:00           95.0            0.52000   \n",
       "\n",
       "     Etat trafic  \n",
       "0         Fluide  \n",
       "1         Fluide  \n",
       "2         Fluide  \n",
       "3         Fluide  \n",
       "4         Fluide  \n",
       "...          ...  \n",
       "8405      Fluide  \n",
       "8406      Fluide  \n",
       "8407      Fluide  \n",
       "8408      Fluide  \n",
       "8409      Fluide  \n",
       "\n",
       "[25184 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_traffic():\n",
    "    sts_df=pd.read_csv('sts.csv',sep=';',parse_dates=['Date debut dispo data','Date et heure de comptage','Date fin dispo data'])\n",
    "    washington_df=pd.read_csv('washington.csv',sep=';',parse_dates=['Date debut dispo data','Date et heure de comptage','Date fin dispo data'])\n",
    "    convention_df=pd.read_csv('convention.csv',sep=';',parse_dates=['Date debut dispo data','Date et heure de comptage','Date fin dispo data'])\n",
    "\n",
    "    data_traffic=pd.concat([sts_df,washington_df,convention_df],axis=0)\n",
    "\n",
    "    data_traffic['Date et heure de comptage']=pd.to_datetime(data_traffic['Date et heure de comptage'], utc=True)\n",
    "    data_traffic['Date et heure de comptage']=data_traffic['Date et heure de comptage'].dt.strftime('%Y-%m-%d %X') \n",
    "    data_traffic['Date et heure de comptage']=pd.to_datetime(data_traffic['Date et heure de comptage'])\n",
    "\n",
    "    data_traffic.drop(columns=[\n",
    "    'Identifiant noeud amont', 'Identifiant noeud aval', 'geo_point_2d',\n",
    "    'geo_shape', 'Date debut dispo data', 'Date fin dispo data','Libelle noeud aval','Libelle noeud amont','Identifiant arc', 'Etat arc'],inplace=True,axis=1)\n",
    "    \n",
    "    return data_traffic\n",
    "\n",
    "extract_traffic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meteo(start, end):\n",
    "    \"\"\"\n",
    "    Extract meteo data from Meteostat API\n",
    "    start: start date\n",
    "    end: end date\n",
    "    \"\"\"\n",
    "    # Create Point for Paris, BC\n",
    "    location = Point(48.856614, 2.333333)\n",
    "\n",
    "    data_=Hourly(location, start, end)\n",
    "    data_=data_.fetch()\n",
    "    #using reset_index() to set index into column\n",
    "    data_=data_.reset_index()\n",
    "    data_.rename( {'time':'date','temp':'temperature', 'prcp':'total_precipitation_in_mm','snow':'snow_depth','tsun':'sunshine_minutes'}, axis=1, inplace=True)\n",
    "    data_=data_[['date', 'temperature', 'total_precipitation_in_mm']]\n",
    "\n",
    "    data_['date']=pd.to_datetime(data_['date'])\n",
    "\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    \"\"\"\n",
    "    Create full dataset\n",
    "    \"\"\"\n",
    "    data_traffic=extract_traffic()\n",
    "    # extract traffic min date and max date\n",
    "    min_date=data_traffic['Date et heure de comptage'].min()\n",
    "    max_date=data_traffic['Date et heure de comptage'].max()\n",
    "\n",
    "    # extract meteo data\n",
    "    meteo=extract_meteo(min_date,max_date)\n",
    "\n",
    "    data=data_traffic.merge(meteo,left_on='Date et heure de comptage',right_on='date',how='left')\n",
    "    data.drop(columns=['date'],inplace=True,axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "# data = create_dataset()\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer(data):\n",
    "    \"\"\"\n",
    "    Engineer features\n",
    "    \"\"\"\n",
    "    data_traffic=data.copy()\n",
    "\n",
    "    # Date\n",
    "    data_traffic['Date']=data_traffic['Date et heure de comptage'].dt.strftime('%Y-%m-%d')\n",
    "    data_traffic['Date']=pd.to_datetime(data_traffic['Date'])\n",
    "    # Time, weekday\n",
    "    data_traffic['Time']=data_traffic['Date et heure de comptage'].dt.strftime('%H').astype(int)\n",
    "    data_traffic['weekday']=data_traffic['Date et heure de comptage'].dt.weekday # Monday=0, Sunday=6\n",
    "    # Holiday\n",
    "    holidays_list=holidays.France(years=[2021, 2022]).keys() # this is a dict\n",
    "    data_traffic.loc[data_traffic['Date'].isin(holidays_list),'holiday']=1\n",
    "    data_traffic['holiday'].fillna(0,inplace=True)\n",
    "    data_traffic.drop(columns=['Date'],inplace=True,axis=1)\n",
    "\n",
    "    return data_traffic\n",
    "\n",
    "# data_engineered=engineer(data)\n",
    "# data_engineered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data):\n",
    "    \"\"\"\n",
    "    Encode data\n",
    "    \"\"\"\n",
    "    data_traffic=data.copy()\n",
    "    \n",
    "    # 'Etat trafic'\n",
    "    mapper = {'Inconnu': 0, 'Fluide': 1, 'Pré-saturé': 2, 'Saturé': 3, 'Bloqué': 4}\n",
    "    data_traffic['Etat trafic'] = data_traffic['Etat trafic'].map(mapper)\n",
    "\n",
    "    # 'weekday', 'Libelle'\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder_df = pd.DataFrame(encoder.fit_transform(data_traffic[['weekday','Libelle']]).toarray(),columns=encoder.get_feature_names_out())\n",
    "    data_traffic = data_traffic.join(encoder_df)\n",
    "\n",
    "    # 'Time'\n",
    "    for i in range(0,24):\n",
    "        data_traffic['time_'+str(i)] = np.where(data_traffic['Time']==i, 1, 0)\n",
    "\n",
    "    # drop columns\n",
    "    data_traffic.drop(columns=['weekday','Libelle','Time'],inplace=True,axis=1)\n",
    "\n",
    "    return data_traffic\n",
    "\n",
    "\n",
    "def preprocess(data, to_encode=True, index=True):\n",
    "    \"\"\"\n",
    "    Preprocess data & Remove Datetime column\n",
    "    \"\"\"\n",
    "    data_traffic=data.copy()\n",
    "\n",
    "    if to_encode:\n",
    "        # Encoded data\n",
    "        data_traffic=encode(data_traffic)\n",
    "\n",
    "    # Fill Misssing values by linear interpolation\n",
    "    data_traffic['Débit horaire']=data_traffic['Débit horaire'].interpolate(method='linear')\n",
    "    data_traffic[\"Taux d'occupation\"]=data_traffic[\"Taux d'occupation\"].interpolate(method='linear')\n",
    "\n",
    "    if index:\n",
    "        # Set index\n",
    "        data_traffic.set_index('Date et heure de comptage',inplace=True)\n",
    "\n",
    "    return data_traffic\n",
    "\n",
    "# data_preprocessed=preprocess(data_engineered, to_encode=False, index=False)\n",
    "# data_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 0 : Baseline model (simple moving average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def baseline(df):\n",
    "#     ## Median value on last 5 weeks\n",
    "#     max_date = df_limit.date_utc.max()\n",
    "#     min_date = max_date - timedelta(days = nb_weeks*7)\n",
    "#     #print(\"max \", max_date)\n",
    "#     #print(\"min \", min_date)\n",
    "#     df_filtered = df_limit[(df_limit.date_utc>min_date) & (df_limit.date_utc<=max_date)]\n",
    "#     #print(df_filtered.weekday_utc.value_counts())\n",
    "#     feature_debit = df_filtered[[\"time_utc\", \"weekday_utc\", \"Débit horaire\"]].groupby([\"weekday_utc\", \"time_utc\"]).median()\n",
    "#     feature_taux = df_filtered[[\"time_utc\", \"weekday_utc\", \"Taux d'occupation\"]].groupby([\"weekday_utc\", \"time_utc\"]).median()\n",
    "#     ## Merge the two features \n",
    "#     features = pd.merge(feature_debit, feature_taux,  how='left', on=['weekday_utc','time_utc'])\n",
    "\n",
    "#     pred = pd.merge(pred, features,  how='left', on=['weekday_utc','time_utc'])\n",
    "\n",
    "#     return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # 'baseline',\n",
    "    LinearRegression(),\n",
    "    RandomForestRegressor(),\n",
    "    xgb.XGBRegressor()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_test(data, start_datetime, nb_rows, model):\n",
    "    \"\"\"\n",
    "    Back test\n",
    "    \"\"\"\n",
    "    # Split train & test\n",
    "    train = data[(data.index < start_datetime)]\n",
    "    test = data[(data.index >= start_datetime) & (data.index < start_datetime + timedelta(hours=nb_rows))]\n",
    "    \n",
    "    y_train = train[['Débit horaire', \"Taux d'occupation\"]]\n",
    "    X_train = train.drop(columns=['Débit horaire', \"Taux d'occupation\"])\n",
    "    \n",
    "    y_test = test[['Débit horaire', \"Taux d'occupation\"]]\n",
    "    X_test = test.drop(columns=['Débit horaire', \"Taux d'occupation\"])\n",
    "\n",
    "    if model == 'baseline':\n",
    "        y_pred = baseline(X_test)\n",
    "\n",
    "    else:\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    mape_debit = mean_absolute_percentage_error(y_test['Débit horaire'], y_pred[:,0])\n",
    "    rmse_debit = mean_squared_error(y_test['Débit horaire'], y_pred[:,0], squared=False)\n",
    "    mape_debit = round(mape_debit, 2)\n",
    "    rmse_debit = round(rmse_debit, 2)\n",
    "    print(\"MAPE Débit horaire: %s\" % (100*mape_debit), '%')\n",
    "    print(\"RMSE Débit horaire: \", rmse_debit)\n",
    "\n",
    "    mape_taux = mean_absolute_percentage_error(y_test[\"Taux d'occupation\"], y_pred[:,1])\n",
    "    rmse_taux = mean_squared_error(y_test[\"Taux d'occupation\"], y_pred[:,1], squared=False)\n",
    "    mape_taux = round(mape_taux, 2)\n",
    "    rmse_taux = round(rmse_taux, 2)\n",
    "    print(\"MAPE Taux d'occupation: %s\" % (100*mape_taux), '%')\n",
    "    print(\"RMSE Taux d'occupation: \", rmse_taux)\n",
    "\n",
    "    df_pred = pd.DataFrame(y_pred, columns=['Débit horaire', \"Taux d'occupation\"], index=y_test.index)\n",
    "    df_pred = df_pred.reset_index()\n",
    "    \n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results : RMSE & MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(start_datetime, model, nb_rows=120):\n",
    "    data = create_dataset()\n",
    "    data_engineered=engineer(data)\n",
    "    data_preprocessed=preprocess(data_engineered, to_encode=False, index=False)\n",
    "    data_preprocessed_index=preprocess(data_engineered, to_encode=False, index=True)\n",
    "    data_preprocessed_encoded=preprocess(data_engineered, to_encode=True, index=True)\n",
    "\n",
    "    df_pred = back_test(data_preprocessed_encoded, start_datetime, nb_rows, model)\n",
    "\n",
    "    # add Libelle column\n",
    "    df_pred = df_pred.merge(data_preprocessed[['Libelle', 'Date et heure de comptage']],  how='left', left_on='Date et heure de comptage', right_on='Date et heure de comptage')\n",
    "\n",
    "    df_pred.sort_values(by='Date et heure de comptage', inplace=True)\n",
    "\n",
    "    # Plot\n",
    "    df_pred_libelle = df_pred[df_pred['Libelle'] == 'Convention'].set_index('Date et heure de comptage')\n",
    "\n",
    "    # df_pred_libelle['Débit horaire'].plot(figsize=(20,5))\n",
    "    # plt.show()\n",
    "    # df_pred_libelle['Taux d\\'occupation'].plot(figsize=(20,5))\n",
    "    # plt.show()\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LinearRegression()\n",
      "MAPE Débit horaire: 45.0 %\n",
      "RMSE Débit horaire:  225.47\n",
      "MAPE Taux d'occupation: 50.0 %\n",
      "RMSE Taux d'occupation:  3.5\n",
      "\n",
      "RandomForestRegressor()\n",
      "MAPE Débit horaire: 40.0 %\n",
      "RMSE Débit horaire:  223.5\n",
      "MAPE Taux d'occupation: 20.0 %\n",
      "RMSE Taux d'occupation:  2.49\n",
      "\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "             early_stopping_rounds=None, enable_categorical=False,\n",
      "             eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
      "             grow_policy='depthwise', importance_type=None,\n",
      "             interaction_constraints='', learning_rate=0.300000012, max_bin=256,\n",
      "             max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
      "             max_depth=6, max_leaves=0, min_child_weight=1, missing=nan,\n",
      "             monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
      "             num_parallel_tree=1, predictor='auto', random_state=0, ...)\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print('')\n",
    "    print(model)\n",
    "    results(datetime(2022, 11, 25, 18, 0, 0), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [131], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_pred \u001b[39m=\u001b[39m results(datetime(\u001b[39m2022\u001b[39;49m, \u001b[39m12\u001b[39;49m, \u001b[39m9\u001b[39;49m, \u001b[39m18\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m), model \u001b[39m=\u001b[39;49m models[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m      3\u001b[0m df_pred\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39moutput_MAIDY.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [130], line 8\u001b[0m, in \u001b[0;36mresults\u001b[1;34m(start_datetime, model, nb_rows)\u001b[0m\n\u001b[0;32m      5\u001b[0m data_preprocessed_index\u001b[39m=\u001b[39mpreprocess(data_engineered, to_encode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m data_preprocessed_encoded\u001b[39m=\u001b[39mpreprocess(data_engineered, to_encode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m df_pred \u001b[39m=\u001b[39m back_test(data_preprocessed_encoded, start_datetime, nb_rows, model)\n\u001b[0;32m     10\u001b[0m \u001b[39m# add Libelle column\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df_pred \u001b[39m=\u001b[39m df_pred\u001b[39m.\u001b[39mmerge(data_preprocessed[[\u001b[39m'\u001b[39m\u001b[39mLibelle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDate et heure de comptage\u001b[39m\u001b[39m'\u001b[39m]],  how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m, left_on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDate et heure de comptage\u001b[39m\u001b[39m'\u001b[39m, right_on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDate et heure de comptage\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [116], line 22\u001b[0m, in \u001b[0;36mback_test\u001b[1;34m(data, start_datetime, nb_rows, model)\u001b[0m\n\u001b[0;32m     19\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     21\u001b[0m \u001b[39m# Evaluate\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m mape_debit \u001b[39m=\u001b[39m mean_absolute_percentage_error(y_test[\u001b[39m'\u001b[39m\u001b[39mDébit horaire\u001b[39m\u001b[39m'\u001b[39m], y_pred[:,\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     23\u001b[0m rmse_debit \u001b[39m=\u001b[39m mean_squared_error(y_test[\u001b[39m'\u001b[39m\u001b[39mDébit horaire\u001b[39m\u001b[39m'\u001b[39m], y_pred[:,\u001b[39m0\u001b[39m], squared\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m mape_debit \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(mape_debit, \u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 1 with size 0"
     ]
    }
   ],
   "source": [
    "df_pred = results(datetime(2022, 12, 9, 18, 0, 0), model = models[-1])\n",
    "\n",
    "df_pred.to_csv('output_MAIDY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f83b22e9e50fa9c0b8f4db8adcdc95fd2379cb2d2f6b89a4b7d887c33051114"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
